# DDPG算法

本文使用DDPG算法来解决无人机数据采集中的路径规划问题。无人机与环境的交互过程为从上一个策略选择中退出，获得该状态的整体即时奖励为 $r_{i-1}$以及下一步的状态信息为$s_{i}$,$s_i$...，进而将当前$s_i$输入到DDPG算法中的动作选择神经网络$Q_{actor}$中，该网络由输入层，隐藏层和输出层组成，第一个全连接网络包含 20000个神经元，其激活函数采用线性整流函数（ReLU, rectified linear unit），第一层网络的输出可以表示为 
$$ X_i = ReLU(W^{T}_{1}s_i+b_i) $$
其中，$W_1$是第一层神经网络的权重参数， $b_1$ 是其 偏差参数,随后的是多层隐藏神经元，最终输出层神经元分为两个具有1280个神经元的块，分别输出加速度变化量和所有物联网设备的选择置信度。最终，根据这些信息选择动作策略，继续与环境交互。

# DDPG训练

在训练 DDPG 算法时，设置无人机的经验池$\left \{ e_k \right \}$  ，其中，$e_k$ 为第


在此之后，利用Actor网络计算出状态s下的动作：然后利用Critic网络计算出状态动作对(s,a_{new})的评估值（即累积期望回报）：采用梯度下降算法优化-q_{new}，从而对Actor网络中的参数进行更新。